{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "colab_type": "code",
        "id": "ajP1Qwu53EOC",
        "outputId": "66f69088-92af-41d9-b58d-49dcada1e457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "4_aLZ0ZW4ZFj",
        "outputId": "85dd6565-42c7-4148-a614-b9188915c149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/c247\n"
          ]
        }
      ],
      "source": [
        "# change path\n",
        "import os\n",
        "\n",
        "try:\n",
        "  project_fname = '/content/drive/My Drive/Colab Notebooks/c247/'\n",
        "  os.chdir(project_fname)\n",
        "except:\n",
        "  project_fname='/content/drive/My Drive/c247'\n",
        "  os.chdir(project_fname)\n",
        "project_data_path = os.path.join(project_fname,'project_data/')\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HHz5sjkWzkUi"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "import numpy as np\n",
        "import random\n",
        "from load_data import * \n",
        "from data_preprocessing import * \n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sklearn import preprocessing\n",
        "\n",
        "X_test, y_test, person_train_valid, X_train_valid, y_train_valid, person_test = load_data(dir_path = project_data_path)\n",
        "\n",
        "# normalize the data\n",
        "N_trials,N_eeg,N_bins,_ = X_train_valid.shape\n",
        "X_train_valid_norm = np.reshape(preprocessing.scale(np.reshape(X_train_valid,(N_trials*N_eeg,N_bins)),axis=1),(N_trials,N_eeg,N_bins,1))\n",
        "N_trials,N_eeg,N_bins,_ = X_test.shape\n",
        "X_test_norm = np.reshape(preprocessing.scale(np.reshape(X_test,(N_trials*N_eeg,N_bins)),axis=1),(N_trials,N_eeg,N_bins,1))\n",
        "\n",
        "\n",
        "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
        "print ('Test data shape: {}'.format(X_test.shape))\n",
        "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
        "print ('Test target shape: {}'.format(y_test.shape))\n",
        "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
        "print ('Person test shape: {}'.format(person_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XYQPBHX0H8LM"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "from scipy import signal \n",
        "import operator\n",
        "\n",
        "# Data preprocessing\n",
        "\n",
        "# Subsample and split into 1 person and 5 EEGs\n",
        "subsample = 5\n",
        "subj = [5]\n",
        "eegs = [0,7,9,11,19]\n",
        "\n",
        "X_train_valid_subsample, y_train_valid_subsample, person_train_valid_subsample = subsample_data(X_train_valid_norm,y_train_valid, person_train_valid, sample_every=subsample)\n",
        "X_train, y_train, person_train = split_data_by_subject(X_train_valid_subsample, y_train_valid_subsample, person_train_valid_subsample)\n",
        "\n",
        "#only get wanted subjects from X_train rather than have all subjects\n",
        "X_train = operator.itemgetter(*subj)(X_train)\n",
        "y_train = operator.itemgetter(*subj)(y_train)\n",
        "\n",
        "X_train = X_train[:,eegs,:,:]\n",
        "\n",
        "print('Shapes: x = {}, y = {}'.format(X_train.shape, y_train.shape))\n",
        "\n",
        "# wavelet transform \n",
        "N_trials,N_eeg,N_bins,_ = X_train.shape\n",
        "fs = 250\n",
        "freq_bins = 50\n",
        "X_train_cwt = morlet_wavelet_transform(X_train,fs=fs,freq_range=(1,20),freq_bins=freq_bins,w=6)\n",
        "# reshape to input to CNN\n",
        "X_train_cwt = np.swapaxes(np.swapaxes(X_train_cwt,1,3),1,2)\n",
        "# scale between -1 and 1 too mimic output of generator\n",
        "X_train_cwt_norm = 2 * (X_train_cwt - np.min(X_train_cwt,axis=0) ) / (np.max(X_train_cwt,axis=0) - np.min(X_train_cwt,axis=0)) - 1\n",
        "\n",
        "print('X_train_cwt_norm = {}'.format(X_train_cwt_norm.shape))\n",
        "\n",
        "print('Deleting original training/validation data from memory...')\n",
        "del X_train_valid_subsample, y_train_valid_subsample, person_train_valid_subsample,\\\n",
        "    X_train, person_train, X_train_valid, y_train_valid, person_train_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "j_yaZdeLnY1-"
      },
      "outputs": [],
      "source": [
        "#load the artificial data and append all of it to the training data\n",
        "split_art_data = 4\n",
        "for task in range(4):\n",
        "  pickle_file = os.path.join(project_data_path,'X_artificial_task{}_subj5_eegs5.npy'.format(task))\n",
        "  trials = np.load(pickle_file)\n",
        "  N=trials.shape[0]\n",
        "  X_train_cwt_norm = np.append(X_train_cwt_norm,trials[:N//split_art_data],axis=0)\n",
        "  y_train = np.append(y_train,np.ones((N//split_art_data))*task)\n",
        "print(X_train_cwt_norm.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nazkH5iNI00d"
      },
      "outputs": [],
      "source": [
        "#Convert test data with CWT exactly the same as X_train_valid\n",
        "\n",
        "X_test_subsample, y_test_subsample, person_test_subsample = subsample_data(X_test_norm,y_test, person_test, sample_every=subsample)\n",
        "X_test, y_test, person_test = split_data_by_subject(X_test_subsample, y_test_subsample, person_test_subsample)\n",
        "\n",
        "#get the specific subjects we want by indexing with the operator module\n",
        "X_test = operator.itemgetter(*subj)(X_test)\n",
        "y_test = operator.itemgetter(*subj)(y_test)\n",
        "X_test = X_test[:,eegs,:,:]\n",
        "\n",
        "# wavelet transform \n",
        "N_trials,N_eeg,N_bins,_ = X_test.shape\n",
        "fs = 250\n",
        "freq_bins = 50\n",
        "X_test_cwt = morlet_wavelet_transform(X_test,fs=fs,freq_range=(1,20),freq_bins=freq_bins,w=6)\n",
        "# reshape to put in same format as the WGAN output\n",
        "X_test_cwt = np.swapaxes(np.swapaxes(X_test_cwt,1,3),1,2)\n",
        "# scale between -1 and 1\n",
        "X_test_cwt_norm = 2 * (X_test_cwt - np.min(X_test_cwt,axis=0) ) / (np.max(X_test_cwt,axis=0) - np.min(X_test_cwt,axis=0)) - 1\n",
        "\n",
        "print('X_cwt_norm = {}'.format(X_test_cwt_norm.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1BqZ-X4S3cnI"
      },
      "outputs": [],
      "source": [
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0sOk3iY73ffu"
      },
      "outputs": [],
      "source": [
        "#import all needed modules\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Input,Dense,Conv2D,ReLU,ELU,\\\n",
        "  Activation,Flatten,AveragePooling2D,Softmax,BatchNormalization,MaxPooling2D,\\\n",
        "  Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,\\\n",
        "  ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZAuUEai9t4B4"
      },
      "source": [
        "CNN for CWT Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DlHLV3CX3h5a"
      },
      "outputs": [],
      "source": [
        "#(50,200,22)\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=32,\n",
        "                   kernel_size=(7,7),\n",
        "                   data_format='channels_last',\n",
        "                   kernel_regularizer=regularizers.l2(0.01),\n",
        "                   activation='relu',\n",
        "                   kernel_initializer='lecun_uniform',\n",
        "                   input_shape=(50,200,5))) #(42,192,32)\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  model.add(MaxPooling2D())#(21,96,32)\n",
        "  model.add(Dropout(rate=0.5))\n",
        "  model.add(Conv2D(filters=32,\n",
        "                   kernel_size=(7,7),\n",
        "                   data_format='channels_last',\n",
        "                   kernel_regularizer=regularizers.l2(0.01),\n",
        "                   activation='relu',\n",
        "                   kernel_initializer='lecun_uniform'))\n",
        "  model.add(BatchNormalization(axis=-1))\n",
        "  model.add(MaxPooling2D()) \n",
        "  model.add(Dropout(rate=0.5))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(750,\n",
        "                  kernel_regularizer=regularizers.l2(0.01),\n",
        "                  activation='relu',\n",
        "                  kernel_initializer='lecun_uniform'))\n",
        "  model.add(Dense(num_classes,\n",
        "                  activation='softmax'))\n",
        "  return model\n",
        "\n",
        "rand_seed = int(datetime.strftime(datetime.now(),\"%Y%m%d%H%M%S\"))\n",
        "batch_size=32\n",
        "num_folds=5\n",
        "num_classes=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oCSVzIIGV-yi"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(learning_rate=1e-5)\n",
        "try:\n",
        "  del model\n",
        "  K.clear_session()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "model = create_model()\n",
        "model.compile(loss=categorical_crossentropy,optimizer=optimizer,metrics=['accuracy'])\n",
        "#plot_model(model,'cnn_cwt_architecture.png',show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bGcGAjzk_2NM"
      },
      "outputs": [],
      "source": [
        "\n",
        "#lr_plateau = ReduceLROnPlateau(patience=4)\n",
        "history = model.fit(x=X_train_cwt_norm,\n",
        "          y=to_categorical(y_train),\n",
        "          epochs=30,\n",
        "          #validation_split=1/num_folds,\n",
        "          validation_data=(X_test_cwt_norm,to_categorical(y_test)),\n",
        "          batch_size=batch_size,\n",
        "          callbacks=[])\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CNN_CWT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
